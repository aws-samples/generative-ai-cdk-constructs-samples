{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6d9036",
   "metadata": {},
   "source": [
    "# SVD model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8145383",
   "metadata": {},
   "source": [
    "Let's start by installing some dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58d35eb-0478-45c0-bbb6-966b404b2d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker==2.226.1\" \"huggingface_hub==0.24.2\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079b84a-5a52-4bbb-a7d9-4070f2767da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it doesn't exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754fc2e",
   "metadata": {},
   "source": [
    "We create a code folder which will contain our custom inference code, and the requirements.txt for additional dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a570eb5-d1a5-4ab9-aed4-39c4025a1dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c59485",
   "metadata": {},
   "source": [
    "Next, we create a requirements.txt file and add the bitsandbytes library to it. The bitsandbytes library is used to quantize the model to 4bit. This library is not available by default in the Hugging Face Inference DLC image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19955c06-29f0-4c07-a264-c678e12e8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "diffusers==0.27.2\n",
    "transformers==4.37.0\n",
    "accelerate==0.27.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6df584",
   "metadata": {},
   "source": [
    "To use custom inference code, we need to create an inference.py script. In our example, we are going to overwrite:\n",
    "- the model_fn to load our svd model correctly \n",
    "- the predict_fn to process incoming requests\n",
    "\n",
    "In the model_fn, we use the StableVideoDiffusionPipeline class from transformers to load the model from the local directory (model_dir).\n",
    "In the predict_fn, we use the generate function from transformers to generate the text for a given text/image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a98174-41e6-4332-a4e8-e2127ec6573c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "import base64\n",
    "import torch\n",
    "from io import BytesIO\n",
    "from diffusers import StableVideoDiffusionPipeline\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \n",
    "    pipe = StableVideoDiffusionPipeline.from_pretrained(model_dir, torch_dtype=torch.float16, variant=\"fp16\")\n",
    "    \n",
    "    pipe.enable_model_cpu_offload()\n",
    "    #pipe.unet.enable_forward_chunking() # https://huggingface.co/docs/diffusers/using-diffusers/svd#reduce-memory-usage\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def predict_fn(data, pipe):\n",
    "    \n",
    "    # get prompt & parameters\n",
    "    prompt = data.pop(\"inputs\", data)\n",
    "    \n",
    "    seed = data.pop(\"seed\", 42)\n",
    "    decode_chunk_size = data.pop(\"decode_chunk_size\", 8)\n",
    "    \n",
    "    image = load_image(prompt)\n",
    "    image = image.resize((1024, 576))\n",
    "\n",
    "    generator = torch.manual_seed(seed)\n",
    "    frames = pipe(image, decode_chunk_size=decode_chunk_size, generator=generator).frames[0]\n",
    "\n",
    "    # create response\n",
    "    encoded_frames = []\n",
    "    for image in frames:\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        encoded_frames.append(base64.b64encode(buffered.getvalue()).decode())\n",
    "\n",
    "    # create response\n",
    "    return {\"frames\": encoded_frames}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007582d2",
   "metadata": {},
   "source": [
    "We now use the huggingface_hub SDK to easily download the stable-video-diffusion-img2vid-xt-1-1 model files from Hugging Face to a model folder. Make sure to replace the value of HF_TOKEN. The model is gated on Hugging Face. To get access, you need to create a user access token. The procedure is detailed here: https://huggingface.co/docs/hub/security-tokens\n",
    "You will need to accept to share you contact information: The model deployed in this sample requires you to agree to share your information before you can access it. Once logged in, visit the [model page](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1) and click on the button 'Agree and access repository'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d6c3db-65b0-4961-86ac-80340150486e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "import random\n",
    "\n",
    "HF_MODEL_ID=\"stabilityai/stable-video-diffusion-img2vid-xt-1-1\"\n",
    "HF_TOKEN=\"REPLACE_WITH_YOUR_TOKEN\"\n",
    "assert len(HF_TOKEN) > 0, \"Please set HF_TOKEN to your huggingface token. You can find it here: https://huggingface.co/settings/tokens\"\n",
    "\n",
    "\n",
    "# download snapshot\n",
    "snapshot_dir = snapshot_download(repo_id=HF_MODEL_ID,use_auth_token=HF_TOKEN)\n",
    "\n",
    "# create model dir\n",
    "model_tar = Path(f\"model-{random.getrandbits(16)}\")\n",
    "model_tar.mkdir(exist_ok=True)\n",
    "\n",
    "# copy snapshot to model dir\n",
    "copy_tree(snapshot_dir, str(model_tar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77085bf4",
   "metadata": {},
   "source": [
    "We copy our custom files (inference.py and requirements.txt) to the model folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe889ab-c81e-4c56-949d-34d3e7dc91ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "# copy code/ to model dir\n",
    "copy_tree(\"code/\", str(model_tar.joinpath(\"code\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5dc559",
   "metadata": {},
   "source": [
    "We create an archive which includes all our files to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbeacf1-aa47-4aca-8729-055bc4c69e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# helper to create the model.tar.gz\n",
    "def compress(tar_dir=None,output_file=\"model.tar.gz\"):\n",
    "    parent_dir=os.getcwd()\n",
    "    os.chdir(tar_dir)\n",
    "    with tarfile.open(os.path.join(parent_dir, output_file), \"w:gz\") as tar:\n",
    "        for item in os.listdir('.'):\n",
    "          print(item)\n",
    "          tar.add(item, arcname=item)\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "compress(str(model_tar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac9fac",
   "metadata": {},
   "source": [
    "Finally, we upload the archive to an Amazon Simple Storage Service bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0b69d-f0bb-4785-9c08-6cd7f5d52150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload model.tar.gz to s3\n",
    "s3_model_uri=S3Uploader.upload(local_path=\"model.tar.gz\", desired_s3_uri=f\"s3://{sess.default_bucket()}/svd-hf-1\")\n",
    "\n",
    "print(f\"model uploaded to: {s3_model_uri}\")\n",
    "# Take note of the s3_model_uri value, this is what the construct will use to deploy the model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

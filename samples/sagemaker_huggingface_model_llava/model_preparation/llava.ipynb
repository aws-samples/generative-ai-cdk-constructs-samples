{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f6d9036",
   "metadata": {},
   "source": [
    "# Llava model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8145383",
   "metadata": {},
   "source": [
    "Let's start by installing some dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58d35eb-0478-45c0-bbb6-966b404b2d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker==2.207.1\" \"huggingface_hub==0.20.3\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079b84a-5a52-4bbb-a7d9-4070f2767da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it doesn't exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754fc2e",
   "metadata": {},
   "source": [
    "We create a code folder which will contain our custom inference code, and the requirements.txt for additional dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a570eb5-d1a5-4ab9-aed4-39c4025a1dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c59485",
   "metadata": {},
   "source": [
    "Next, we create a requirements.txt file and add the bitsandbytes library to it. The bitsandbytes library is used to quantize the model to 4bit. This library is not available by default in the Hugging Face Inference DLC image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19955c06-29f0-4c07-a264-c678e12e8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "bitsandbytes==0.42.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6df584",
   "metadata": {},
   "source": [
    "To use custom inference code, we need to create an inference.py script. In our example, we are going to overwrite:\n",
    "- the model_fn to load our llava model correctly \n",
    "- the predict_fn to process incoming requests\n",
    "\n",
    "In the model_fn, we use the LlavaForConditionalGeneration class from transformers to load the model from the local directory (model_dir). We specify device_map=\"auto\" in order to automatically place the model on the available GPUs/CPUs (see this [guide](https://huggingface.co/docs/accelerate/usage_guides/big_modeling) for details). We also enable 4-bit inference (see [this blog](https://huggingface.co/blog/4bit-transformers-bitsandbytes) for details).\n",
    "In the predict_fn, we use the generate function from transformers to generate the text for a given text/image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a98174-41e6-4332-a4e8-e2127ec6573c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model_dir)\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(model_dir, quantization_config=quantization_config, device_map=\"auto\")\n",
    "\n",
    "    return processor, model\n",
    "\n",
    "\n",
    "def predict_fn(data, processor_and_model):\n",
    "    \n",
    "    processor, model = processor_and_model\n",
    "    \n",
    "    # get prompt & parameters\n",
    "    prompt = data.pop(\"question\", data)\n",
    "    image_path = data.pop(\"image\", data)\n",
    "    max_new_tokens = data.pop(\"max_new_tokens\", 200)\n",
    "    \n",
    "    image = Image.open(requests.get(image_path, stream=True).raw)\n",
    "    \n",
    "    inputs = processor(prompt, images=[image], padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    output = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    generated_text = processor.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "    # create response\n",
    "    return {\"output\": generated_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007582d2",
   "metadata": {},
   "source": [
    "We now use the huggingface_hub SDK to easily download the llava-1.5-7b-hf model files from Hugging Face to a model folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d6c3db-65b0-4961-86ac-80340150486e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "import random\n",
    "\n",
    "HF_MODEL_ID=\"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "# download snapshot\n",
    "snapshot_dir = snapshot_download(repo_id=HF_MODEL_ID)\n",
    "\n",
    "# create model dir\n",
    "model_tar = Path(f\"model-{random.getrandbits(16)}\")\n",
    "model_tar.mkdir(exist_ok=True)\n",
    "\n",
    "# copy snapshot to model dir\n",
    "copy_tree(snapshot_dir, str(model_tar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77085bf4",
   "metadata": {},
   "source": [
    "We copy our custom files (inference.py and requirements.txt) to the model folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe889ab-c81e-4c56-949d-34d3e7dc91ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "# copy code/ to model dir\n",
    "copy_tree(\"code/\", str(model_tar.joinpath(\"code\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5dc559",
   "metadata": {},
   "source": [
    "We create an archive which includes all our files to run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbeacf1-aa47-4aca-8729-055bc4c69e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# helper to create the model.tar.gz\n",
    "def compress(tar_dir=None,output_file=\"model.tar.gz\"):\n",
    "    parent_dir=os.getcwd()\n",
    "    os.chdir(tar_dir)\n",
    "    with tarfile.open(os.path.join(parent_dir, output_file), \"w:gz\") as tar:\n",
    "        for item in os.listdir('.'):\n",
    "          print(item)\n",
    "          tar.add(item, arcname=item)\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "compress(str(model_tar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac9fac",
   "metadata": {},
   "source": [
    "Finally, we upload the archive to an Amazon Simple Storage Service bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0b69d-f0bb-4785-9c08-6cd7f5d52150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload model.tar.gz to s3\n",
    "s3_model_uri=S3Uploader.upload(local_path=\"model.tar.gz\", desired_s3_uri=f\"s3://{sess.default_bucket()}/llava-hf-15-7b-test1\")\n",
    "\n",
    "print(f\"model uploaded to: {s3_model_uri}\")\n",
    "# Take note of the s3_model_uri value, this is what the construct will use to deploy the model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
